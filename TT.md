# Задание для разработчиков (Java)

## Цель
Проверить полученные знания на практике, создав ReadProcessWrite приложение для генерации real-time статистики продаж.

## Задача
Давайте представим, что у нас есть успешный интернет-магазин. На прошлой неделе наши инженеры установили кластер Apache Kafka, в топик которого отправляются сообщения о всех проведённых оплатах на нашем сайте.

Наша задача — создать аналитическое приложение, которое бы считывало успешные платежи из топика (см. флаг "isSuccessful") и агрегировало их суммы в поминутные срезы (i.o.w. поминутные окна, Tumbling Window). Подсчитанные агрегации должны отправляться в отдельный топик.

Постарайтесь подойти к задаче итеративно, двигаясь от простого решения к более сложному — кажущаяся простота скрывает под собой массу вопросов связанных с обработкой времени. Для решения подобного класса задач созданы целые фреймворки, например Kafka Streams или Apache Flink!

- В качестве генератора случайных оплат вы можете воспользоваться классом `io.slurm.kafka.TestProducer`.
- Вы также можете воспользоваться Docker для запуска кластера Apache Kafka локально.
- В помощь есть пример `ReadWriteApp` приложения.
- Подумайте, каким образом вы будете высчитывать границы минутного окна? Можем ли мы использовать для этого локальное время нашего приложения (Processing Time), какие плюсы и минусы есть у этого подхода по сравнению с использованием времени отправки/генерации самих сообщений (Event Time)?
- Для упрощения задачи, предположим, что у в исходном топике оплат не бывает out-of-order или late сообщений. Мы можем гарантировать это выставив конфигурационную опцию топика `message.timestamp.type` в `LogAppendTime`.
- Каким образом мы можем гарантировать корректность репорта: например, отсутствие повторных обработок одних и тех же сообщений или потерь данных? При желании, попробуйте так же имплементировать репорт используя библиотеку Kafka Streams.
